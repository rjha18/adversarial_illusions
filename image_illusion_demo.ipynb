{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d45aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishi/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/rishi/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/rishi/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import threshold, criterion, unnorm\n",
    "from dataset_utils import imagenet_loader\n",
    "from models import load_model\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Configure Script\n",
    "gpu_num = 1\n",
    "epochs = [100, 200, 300, 500, 1000]\n",
    "batch_size = 4\n",
    "eps = 16 / 255\n",
    "lr = 0.02\n",
    "n_images = 100\n",
    "model_flag = 'imagebind'\n",
    "gamma_epochs = 100\n",
    "modality = 'thermal'\n",
    "\n",
    "if type(epochs) == list:\n",
    "    max_epochs = max(epochs)\n",
    "else:\n",
    "    max_epochs = epochs\n",
    "    epochs = [epochs]\n",
    "\n",
    "device = f\"cuda:{gpu_num}\" if torch.cuda.is_available() and gpu_num >= 0 else \"cpu\"\n",
    "assert n_images % batch_size == 0\n",
    "\n",
    "# Instantiate Model\n",
    "model = load_model(model_flag, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac901e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_transform_thermal_data(image_paths, device):\n",
    "    if image_paths is None:\n",
    "        return None\n",
    "\n",
    "    image_outputs = []\n",
    "    for image_path in image_paths:\n",
    "        data_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    224, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                ),\n",
    "                transforms.Grayscale()\n",
    "            ]\n",
    "        )\n",
    "        with open(image_path, \"rb\") as fopen:\n",
    "            image = Image.open(fopen).convert(\"RGB\")\n",
    "\n",
    "        image = data_transform(image).to(device)\n",
    "        image_outputs.append(image)\n",
    "    return torch.stack(image_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4efb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224]) torch.Size([1, 1, 224, 224]) torch.Size([1, 1, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:04<?, ?it/s, loss=tensor([0.9595]), eta=0.02]/home/rishi/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      " 20%|█▉        | 396/2000 [00:13<00:56, 28.32it/s, loss=tensor([0.3746]), eta=0.0146]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     31\u001b[0m     eta \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X, modality, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m     cton \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m criterion(embeds, Y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m criterion(embeds, Y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/code/adversarial_illusions/models.py:49\u001b[0m, in \u001b[0;36mImageBindWrapper.forward\u001b[0;34m(self, X, modality, normalize)\u001b[0m\n\u001b[1;32m     47\u001b[0m     X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_and_transform_text(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     48\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward({modality: X}, normalize\u001b[38;5;241m=\u001b[39mnormalize)[modality]\n",
      "File \u001b[0;32m~/code/adversarial_illusions/imagebind/models/imagebind_model.py:462\u001b[0m, in \u001b[0;36mImageBindModel.forward\u001b[0;34m(self, inputs, normalize)\u001b[0m\n\u001b[1;32m    460\u001b[0m trunk_inputs \u001b[38;5;241m=\u001b[39m modality_value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrunk\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    461\u001b[0m head_inputs \u001b[38;5;241m=\u001b[39m modality_value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 462\u001b[0m modality_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_trunks[modality_key](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrunk_inputs)\n\u001b[1;32m    463\u001b[0m modality_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_heads[modality_key](\n\u001b[1;32m    464\u001b[0m     modality_value, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhead_inputs\n\u001b[1;32m    465\u001b[0m )\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "File \u001b[0;32m~/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/adversarial_illusions/imagebind/models/transformer.py:277\u001b[0m, in \u001b[0;36mSimpleTransformer.forward\u001b[0;34m(self, tokens, attn_mask, use_checkpoint, checkpoint_every_n, checkpoint_blk_ids)\u001b[0m\n\u001b[1;32m    273\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    274\u001b[0m             blk, tokens, attn_mask, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m blk(tokens, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer:\n\u001b[1;32m    279\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_transformer_layer(tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/adversarial_illusions/imagebind/models/transformer.py:162\u001b[0m, in \u001b[0;36mBlockWithMasking.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(x), attn_mask))\n\u001b[0;32m--> 162\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_2(x)))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m         x\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(x), attn_mask))\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_gamma1\n\u001b[1;32m    168\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[0;32m~/anaconda3/envs/adv_collisions/lib/python3.11/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the paths for the original and perturbed folders\n",
    "original_folder = '../data/thermal'\n",
    "perturbed_folder = 'outputs/thermal_perturbed'\n",
    "\n",
    "# Get the list of image files in the original folder\n",
    "image_files = [file for file in os.listdir(original_folder) if file.endswith('.jpg')]\n",
    "\n",
    "# Iterate over each image file\n",
    "for image_file in image_files:\n",
    "    # Load the original image\n",
    "    perturbed_image_path = os.path.join(original_folder, image_file)\n",
    "    X = load_and_transform_thermal_data([perturbed_image_path], device)\n",
    "\n",
    "    X_init = X.clone().detach().cpu().requires_grad_(False)\n",
    "    target_text=[\"murderer with gun\",]\n",
    "    Y = model.forward(target_text, \"text\", normalize=False)\n",
    "    X, Y = X.to(device).requires_grad_(True), Y.to(device)\n",
    "\n",
    "    max_epochs=2000\n",
    "    pbar = tqdm(range(max_epochs))\n",
    "    X_init = X.clone().detach().cpu().requires_grad_(False)\n",
    "    X, Y = X.to(device).requires_grad_(True), Y.to(device)\n",
    "    X_max, X_min = threshold(X, eps, modality, device)\n",
    "    optimizer = optim.SGD([X], lr=lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                np.arange(gamma_epochs, max_epochs, gamma_epochs),\n",
    "                                                gamma=0.9)\n",
    "    for j in pbar:\n",
    "        eta = scheduler.get_last_lr()[0]\n",
    "        embeds = model.forward(X, modality, normalize=False)\n",
    "        cton = 1 - criterion(embeds, Y, dim=1).detach().cpu()\n",
    "        loss = 1 - criterion(embeds, Y, dim=1)\n",
    "        update = eta * torch.autograd.grad(outputs=loss.mean(), inputs=X)[0].sign()\n",
    "        X = (X.detach().cpu() - update.detach().cpu()).to(device)\n",
    "        X = torch.clamp(X, min=X_min, max=X_max).requires_grad_(True)\n",
    "        pbar.set_postfix({'loss': cton, 'eta': eta})\n",
    "        scheduler.step()\n",
    "\n",
    "    perturbed_image_path = os.path.join(perturbed_folder, image_file.replace('.jpg','.png'))\n",
    "    save_image(unnorm(torch.squeeze(X.cuda()))[0], perturbed_image_path)\n",
    "\n",
    "print(\"Perturbation completed and images saved to the perturbed folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4e498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
