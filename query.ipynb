{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import toml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import norm, unnorm, criterion\n",
    "from dataset_utils import create_dataset\n",
    "from models import load_model\n",
    "import time\n",
    "\n",
    "# Configure Script\n",
    "config = toml.load(f'configs/imagenet/query/audioclip.toml')['general']\n",
    "epochs = config['epochs']\n",
    "gpu_num = config['gpu_num']\n",
    "batch_size = config['batch_size']\n",
    "eps = config['epsilon']\n",
    "seed = config['seed']\n",
    "output_dir = config['output_dir']\n",
    "n_images = config['number_images']\n",
    "buffer_size = config['buffer_size']\n",
    "delta = config['delta']\n",
    "model_flag = config.get('model_flag', 'imagebind')\n",
    "embs_input = config.get('embeddings_input', output_dir + 'embs.npy')\\\n",
    "                   .format(model_flag)\n",
    "modality = config.get('modality', 'vision')\n",
    "dataset_flag = config.get('dataset_flag', 'imagenet')\n",
    "input_images_file = config.get('input_images_file', None)\n",
    "\n",
    "if modality == 'vision':\n",
    "    eps = eps / 255\n",
    "    \n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "full_flag=False\n",
    "if \"full\" in output_dir:\n",
    "    full_flag = True\n",
    "\n",
    "print('Full_flag: ',full_flag)\n",
    "\n",
    "hybrid=False\n",
    "if input_images_file!=None:\n",
    "    hybrid=True\n",
    "print('Hybrid: ',hybrid)\n",
    "\n",
    "device = f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert n_images % batch_size == 0\n",
    "\n",
    "# Instantiate Model\n",
    "model = load_model(model_flag, device)\n",
    "\n",
    "# Load Data\n",
    "image_text_dataset = create_dataset(dataset_flag, model=model, device=device, seed=seed, embs_input=embs_input)\n",
    "# Create Adversarial Examples\n",
    "X_advs = []\n",
    "X_inits = []\n",
    "gts = []\n",
    "gt_loss = []\n",
    "adv_loss = []\n",
    "end_iter = []\n",
    "\n",
    "# TODO: verify added code\n",
    "y_ids = []\n",
    "y_origs = []\n",
    "\n",
    "final = []\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def get_loss(y, logits, targeted=False, loss_type='margin_loss'):\n",
    "    \"\"\" Implements the margin loss (difference between the correct and 2nd best class). \"\"\"\n",
    "    if loss_type == 'margin_loss':\n",
    "        preds_correct_class = (logits * y).sum(1, keepdims=True)\n",
    "        diff = preds_correct_class - logits  # difference between the correct class and all other classes\n",
    "        diff[y] = np.inf  # to exclude zeros coming from f_correct - f_correct\n",
    "        margin = diff.min(1, keepdims=True)\n",
    "        loss = margin * -1 if targeted else margin\n",
    "        # print(loss)\n",
    "    elif loss_type == 'cross_entropy':\n",
    "        probs = softmax(logits)\n",
    "        # print(y)\n",
    "        # print(probs.shape)\n",
    "        loss = -np.log(probs[y])\n",
    "        loss = loss * -1 if not targeted else loss\n",
    "    return loss.flatten()\n",
    "    \n",
    "def dense_to_onehot(y_test, n_cls):\n",
    "    y_test_onehot = np.zeros([len(y_test), n_cls], dtype=bool)\n",
    "    y_test_onehot[np.arange(len(y_test)), y_test] = True\n",
    "    return y_test_onehot\n",
    "\n",
    "def p_selection(p_init, it, n_iters):\n",
    "    \"\"\" Piece-wise constant schedule for p (the fraction of pixels changed on every iteration). \"\"\"\n",
    "    it = int(it / n_iters * 10000)\n",
    "\n",
    "    if 10 < it <= 50:\n",
    "        p = p_init / 2\n",
    "    elif 50 < it <= 200:\n",
    "        p = p_init / 4\n",
    "    elif 200 < it <= 500:\n",
    "        p = p_init / 8\n",
    "    elif 500 < it <= 1000:\n",
    "        p = p_init / 16\n",
    "    elif 1000 < it <= 2000:\n",
    "        p = p_init / 32\n",
    "    elif 2000 < it <= 4000:\n",
    "        p = p_init / 64\n",
    "    elif 4000 < it <= 6000:\n",
    "        p = p_init / 128\n",
    "    elif 6000 < it <= 8000:\n",
    "        p = p_init / 256\n",
    "    elif 8000 < it <= 10000:\n",
    "        p = p_init / 512\n",
    "    else:\n",
    "        p = p_init\n",
    "\n",
    "    return p\n",
    "\n",
    "def square_attack_linf(model, x, y, eps, n_iters, p_init, metrics_path, targeted, loss_type, local_adv=None):\n",
    "    \"\"\" The Linf square attack \"\"\"\n",
    "    np.random.seed(0)  # important to leave it here as well\n",
    "    early_break=False\n",
    "    x=unnorm(x).to(device)\n",
    "    min_val, max_val = 0, 1\n",
    "    c, h, w = x.shape[1:]\n",
    "    n_features = c*h*w\n",
    "    n_ex_total = x.shape[0]\n",
    "\n",
    "    if local_adv==None:\n",
    "        init_delta = torch.tensor(np.random.choice([-eps, eps], size=[x.shape[0], c, 1, w])).to(torch.float).to(device)\n",
    "    else:\n",
    "        local_adv=unnorm(local_adv).to(device)\n",
    "        init_delta = local_adv-x\n",
    "    # init_delta=0\n",
    "    x_best = torch.clip(x + init_delta, min_val, max_val)\n",
    "    with torch.no_grad():\n",
    "        embeds = model.forward(x_best.cuda(), modality, normalize=True)\n",
    "    logits=criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].cpu(), dim=2).detach().cpu().numpy()\n",
    "    loss_min = get_loss(y, logits, targeted, loss_type=loss_type)\n",
    "    margin_min = get_loss(y, logits, targeted, loss_type='margin_loss')\n",
    "    n_queries = np.ones(x.shape[0])  # ones because we have already used 1 query\n",
    "\n",
    "    time_start = time.time()\n",
    "    metrics = np.zeros([n_iters, 7])\n",
    "    for i_iter in range(n_iters - 1):\n",
    "        idx_to_fool = margin_min > 0\n",
    "        x_curr, x_best_curr, y_curr = x[idx_to_fool], x_best[idx_to_fool], y[idx_to_fool]\n",
    "        loss_min_curr, margin_min_curr = loss_min[idx_to_fool], margin_min[idx_to_fool]\n",
    "        deltas = x_best_curr - x_curr\n",
    "        p = p_selection(p_init, i_iter, n_iters)\n",
    "        for i_img in range(x_best_curr.shape[0]):\n",
    "            s = int(round(np.sqrt(p * n_features / c)))\n",
    "            s = min(max(s, 1), h-1)  # at least c x 1 x 1 window is taken and at most c x h-1 x h-1\n",
    "            center_h = np.random.randint(0, h - s)\n",
    "            center_w = np.random.randint(0, w - s)\n",
    "\n",
    "            x_curr_window = x_curr[i_img, :, center_h:center_h+s, center_w:center_w+s]\n",
    "            x_best_curr_window = x_best_curr[i_img, :, center_h:center_h+s, center_w:center_w+s]\n",
    "            # prevent trying out a delta if it doesn't change x_curr (e.g. an overlapping patch)\n",
    "            while torch.sum(torch.abs(torch.clip(x_curr_window + deltas[i_img, :, center_h:center_h+s, center_w:center_w+s], min_val, max_val) - x_best_curr_window) < 10**-7) == c*s*s:\n",
    "                deltas[i_img, :, center_h:center_h+s, center_w:center_w+s] = torch.tensor(np.random.choice([-eps, eps], size=[c, 1, 1]))\n",
    "\n",
    "        x_new = torch.clip(x_curr + deltas, min_val, max_val).to(device)\n",
    "        with torch.no_grad():\n",
    "            embeds = model.forward(x_new, modality, normalize=True)\n",
    "        logits=criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].cpu(), dim=2).detach().cpu().numpy()\n",
    "        loss = get_loss(y_curr, logits, targeted, loss_type=loss_type)\n",
    "        margin = get_loss(y_curr, logits, targeted, loss_type='margin_loss')\n",
    "\n",
    "        \n",
    "        idx_improved = loss < loss_min_curr\n",
    "        loss_min[idx_to_fool] = idx_improved * loss + ~idx_improved * loss_min_curr\n",
    "        margin_min[idx_to_fool] = idx_improved * margin + ~idx_improved * margin_min_curr\n",
    "        idx_improved = torch.tensor(np.reshape(idx_improved, [-1, *[1]*len(x.shape[:-1])])).to(device)\n",
    "        x_best[idx_to_fool] = idx_improved * x_new + ~idx_improved * x_best_curr\n",
    "        n_queries[idx_to_fool] += 1\n",
    "        acc = (margin_min > 0.0).sum() / n_ex_total\n",
    "        acc_corr = (margin_min > 0.0).mean()\n",
    "        mean_nq, mean_nq_ae, median_nq_ae = np.mean(n_queries), np.mean(n_queries[margin_min <= 0]), np.median(n_queries[margin_min <= 0])\n",
    "        avg_margin_min = np.mean(margin_min)\n",
    "        time_total = time.time() - time_start\n",
    "        if i_iter%10==0:\n",
    "            print('{}: acc={:.2%} acc_corr={:.2%} avg#q_ae={:.2f} med#q={:.1f}, avg_margin={:.6f} (n_ex={}, eps={:.3f}, {:.2f}s)'.\n",
    "                format(i_iter+1, acc, acc_corr, mean_nq_ae, median_nq_ae, avg_margin_min, x.shape[0], eps, time_total))\n",
    "\n",
    "        metrics[i_iter] = [acc, acc_corr, mean_nq, mean_nq_ae, median_nq_ae, margin_min.mean(), time_total]\n",
    "            \n",
    "        if i_iter>10000 and (metrics[i_iter-10000][5]-metrics[i_iter][5]<0.001):\n",
    "            early_break=True\n",
    "        if (i_iter <= 500 and i_iter % 20 == 0) or (i_iter > 100 and i_iter % 50 == 0) or i_iter + 1 == n_iters or acc == 0 or early_break==True:\n",
    "            np.save(metrics_path, metrics)\n",
    "\n",
    "        if full_flag==False:\n",
    "            if acc == 0 or early_break==True:\n",
    "                break\n",
    "        else:\n",
    "            if early_break==True:\n",
    "                break\n",
    "\n",
    "    return n_queries, x_best\n",
    "\n",
    "targeted=True  \n",
    "p=0.05\n",
    "if targeted:\n",
    "    p=0.01\n",
    "loss='cross_entropy'\n",
    "torch.manual_seed(seed)\n",
    "if hybrid==True:\n",
    "    local_advs_path = input_images_file\n",
    "    local_advs = np.load(local_advs_path)\n",
    "    local_advs=torch.tensor(local_advs).to(device)\n",
    "\n",
    "    success=0\n",
    "    n=0\n",
    "    dataloader = DataLoader(image_text_dataset, batch_size=1, shuffle=True)\n",
    "    for i, (X, Y, gt, y_id, y_orig) in enumerate(dataloader):\n",
    "        if n==100:\n",
    "            break\n",
    "        embeds = model.forward(local_advs[i].unsqueeze(0), modality, normalize=False)\n",
    "        classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "        if torch.all((classes == y_id[:, None]).nonzero(as_tuple=True)[1].cpu() == 0):\n",
    "            success+=1\n",
    "        n+=1\n",
    "    print(\"the attack success rate is \"+ str(success)+ \"%\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "dataloader = DataLoader(image_text_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs=200\n",
    "eps=0.2\n",
    "for i, (X, Y, gt, y_id, y_orig) in enumerate(dataloader):\n",
    "    print(i)\n",
    "    metrics_path = f\"{output_dir}metrics_{i}.npy\"\n",
    "    if targeted:\n",
    "        y=dense_to_onehot(y_id, n_cls=1000)\n",
    "    else:\n",
    "        y=dense_to_onehot(y_orig, n_cls=1000)\n",
    "\n",
    "    if hybrid==True:\n",
    "        local_adv=local_advs[i*batch_size:i*batch_size+batch_size]\n",
    "        n_queries, x_adv = square_attack_linf(model, X, y, eps, epochs,\n",
    "                                        p, metrics_path, targeted, loss, local_adv)\n",
    "    else:\n",
    "        n_queries, x_adv = square_attack_linf(model, X, y, eps, epochs,\n",
    "                                        p, metrics_path, targeted, loss)\n",
    "        print(\"hello\")\n",
    "    if i == (n_images // batch_size)-1:\n",
    "        break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeds = model.forward(x_adv.cuda(), modality, normalize=True)\n",
    "logits=criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].cpu(), dim=2).detach().cpu().numpy()\n",
    "y=dense_to_onehot(y_id, n_cls=1000)\n",
    "loss_min = get_loss(y, logits, targeted, loss_type='margin_loss')\n",
    "print(loss_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeds = model.forward(x_adv.to(device), modality, normalize=True).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "print((classes == y_id[:, None])[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.ToPILImage()\n",
    "plt.imshow(transform(torch.squeeze(unnorm(gt)[0])))\n",
    "plt.show()\n",
    "plt.imshow(transform(torch.squeeze(x_adv[0])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_adv.cuda()-).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtunnorm=unnorm(gt.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_adv-gtunnorm.cuda()).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeds = model.forward(gt.to(device), modality, normalize=False).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "print((classes == y_id[:, None])[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record batchwise information\n",
    "for i, (X, Y, gt, y_id, y_orig) in enumerate(dataloader):\n",
    "    embeds = model.forward(gt.to(device), modality, normalize=False).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "    print((classes == y_id[:, None])[:, 0].sum())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeds = model.forward(x_adv.cuda(), modality, normalize=False)\n",
    "logits=criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].cpu(), dim=2).detach().cpu().numpy()\n",
    "y=dense_to_onehot(y_id, n_cls=1000)\n",
    "loss_min = get_loss(y, logits, targeted, loss_type='margin_loss')\n",
    "print(loss_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y, logits, targeted=False, loss_type='margin_loss'):\n",
    "    \"\"\" Implements the margin loss (difference between the correct and 2nd best class). \"\"\"\n",
    "    if loss_type == 'margin_loss':\n",
    "        preds_correct_class = (logits * y).sum(1, keepdims=True)\n",
    "        diff = preds_correct_class - logits  # difference between the correct class and all other classes\n",
    "        diff[y] = np.inf  # to exclude zeros coming from f_correct - f_correct\n",
    "        margin = diff.min(1, keepdims=True)\n",
    "        loss = margin * -1 if targeted else margin\n",
    "        # print(loss)\n",
    "    elif loss_type == 'cross_entropy':\n",
    "        probs = softmax(logits)\n",
    "        # print(y)\n",
    "        # print(probs.shape)\n",
    "        loss = -np.log(probs[y])\n",
    "        loss = loss * -1 if not targeted else loss\n",
    "    return loss.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record batchwise information\n",
    "with torch.no_grad():\n",
    "    norm_x_adv=norm(x_adv)\n",
    "    gt_embeddings = model.forward(norm_x_adv.to(device), modality, normalize=True).detach().cpu()\n",
    "    embeds = model.forward(x_adv.to(device), modality, normalize=True).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "gt_loss = []\n",
    "\n",
    "X_advs.append(norm_x_adv.detach().cpu().clone())\n",
    "X_inits.append(X.cpu().clone())\n",
    "gts.append(gt.cpu().clone())\n",
    "gt_loss.append(criterion(gt_embeddings, Y.cpu(), dim=1))\n",
    "adv_loss.append(criterion(embeds.detach().cpu(), Y.cpu(), dim=1))\n",
    "end_iter.append(n_queries)\n",
    "print((classes == y_id[:, None])[:, 0])\n",
    "y_ids.append(y_id.cpu())\n",
    "y_origs.append(y_orig.cpu())\n",
    "final.append((classes == y_id[:, None])[:, 0].cpu())\n",
    "\n",
    "np.save(output_dir + 'x_advs', np.concatenate(X_advs))\n",
    "np.save(output_dir + 'x_inits', np.concatenate(X_inits))\n",
    "np.save(output_dir + 'gts', np.concatenate(gts))\n",
    "np.save(output_dir + 'gt_loss', np.concatenate(gt_loss))\n",
    "np.save(output_dir + 'adv_loss', np.concatenate(adv_loss))\n",
    "np.save(output_dir + 'end_iter', np.concatenate(end_iter))\n",
    "\n",
    "np.save(output_dir + 'y_ids', np.concatenate(y_ids))\n",
    "np.save(output_dir + 'y_origs', np.concatenate(y_origs))\n",
    "np.save(output_dir + 'final', np.concatenate(final))\n",
    "\n",
    "# Compute and print the average and standard deviation of gt_loss and adv_loss\n",
    "gt_loss_avg = np.mean(np.concatenate(gt_loss))\n",
    "gt_loss_std = np.std(np.concatenate(gt_loss))\n",
    "adv_loss_avg = np.mean(np.concatenate(gt_loss))\n",
    "adv_loss_std = np.std(np.concatenate(gt_loss))\n",
    "\n",
    "print(\"Average organic alignment:\", gt_loss_avg)\n",
    "print(\"Standard deviation of organic alignment:\", gt_loss_std)\n",
    "print(\"Average adversarial alignment:\", adv_loss_avg)\n",
    "print(\"Standard deviation of adversarial alignment:\", adv_loss_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loss.append(criterion(embeds.detach().cpu(), Y.cpu(), dim=1))\n",
    "\n",
    "# Compute and print the average and standard deviation of gt_loss and adv_loss\n",
    "gt_loss_avg = np.mean(np.concatenate(gt_loss))\n",
    "gt_loss_std = np.std(np.concatenate(gt_loss))\n",
    "adv_loss_avg = np.mean(np.concatenate(gt_loss))\n",
    "adv_loss_std = np.std(np.concatenate(gt_loss))\n",
    "\n",
    "print(\"Average organic alignment:\", gt_loss_avg)\n",
    "print(\"Standard deviation of organic alignment:\", gt_loss_std)\n",
    "print(\"Average adversarial alignment:\", adv_loss_avg)\n",
    "print(\"Standard deviation of adversarial alignment:\", adv_loss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(gt_embeddings[:, None, :].cpu(), Y.cpu(), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/adversarial_illusions/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/tz362/anaconda3/envs/adversarial_illusions/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m x_adv_path \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/imagenet/whitebox/imagebind_jpeg/x_advs_300.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     54\u001b[0m y_id_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/imagenet/whitebox/imagebind_jpeg/y_ids.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mevaluate_jpeg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_adv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_id_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_text_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36mevaluate_jpeg\u001b[0;34m(model, x_adv_path, y_id_path, modality, image_text_dataset)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_jpeg\u001b[39m(model, x_adv_path, y_id_path, modality, image_text_dataset):\n\u001b[0;32m---> 36\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m()\n\u001b[1;32m     37\u001b[0m     y_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(y_id_path)\u001b[38;5;241m.\u001b[39mtensor()\n\u001b[1;32m     38\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x_adv\u001b[38;5;241m.\u001b[39mto(device), modality, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tensor'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(88)\n",
      "tensor(91)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_transform(X):\n",
    "    batch_size = X.shape[0]\n",
    "    transformed_images = []\n",
    "    for i in range(batch_size):\n",
    "        image_filename = f'dummy_{i}.jpg'\n",
    "        save_image(torch.squeeze(unnorm(X[i])), image_filename)\n",
    "        jpeg_X = data.load_and_transform_vision_data([image_filename], device)\n",
    "        os.remove(image_filename)\n",
    "        transformed_images.append(jpeg_X.squeeze().cpu())\n",
    "    # Stack the transformed images back into a single tensor\n",
    "    jpeg_X_batch = torch.stack(transformed_images)\n",
    "    return jpeg_X_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import toml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import norm, unnorm, criterion\n",
    "from dataset_utils import create_dataset\n",
    "from models import load_model\n",
    "import time\n",
    "import imagebind.data as data\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device='cuda:0'\n",
    "dataset_flag='imagenet'\n",
    "seed=0\n",
    "\n",
    "def jpeg_transform(X, unnorm, device):\n",
    "    batch_size = X.shape[0]\n",
    "    transformed_images = []\n",
    "    for i in range(batch_size):\n",
    "        image_filename = f'dummy_{i}.jpg'\n",
    "        save_image(torch.squeeze(unnorm(X[i])), image_filename)\n",
    "        jpeg_X = data.load_and_transform_vision_data([image_filename], device)\n",
    "        os.remove(image_filename)\n",
    "        transformed_images.append(jpeg_X.squeeze().cpu())\n",
    "    # Stack the transformed images back into a single tensor\n",
    "    jpeg_X_batch = torch.stack(transformed_images)\n",
    "    return jpeg_X_batch\n",
    "\n",
    "\n",
    "def evaluate_jpeg(model, x_adv_path, y_id_path, modality, image_text_dataset):\n",
    "    x_adv = torch.from_numpy(np.load(x_adv_path))\n",
    "    y_id = torch.from_numpy(np.load(y_id_path))\n",
    "    with torch.no_grad():\n",
    "        embeds = model.forward(x_adv.to(device), modality, normalize=False).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "    print((classes == y_id[:, None])[:, 0].sum().item()+'\\%')\n",
    "    # attack success rate with jpeg\n",
    "    with torch.no_grad():\n",
    "        embeds = model.forward(jpeg_transform(x_adv).to(device), modality, normalize=False).detach().cpu()\n",
    "    classes = criterion(embeds[:, None, :].cpu(), image_text_dataset.labels[None, :, :].detach().cpu(), dim=2).argsort(dim=1, descending=True)\n",
    "    print((classes == y_id[:, None])[:, 0].sum()+'\\%')\n",
    "\n",
    "\n",
    "model_flag = 'imagebind'\n",
    "embs_input = 'outputs/embeddings/imagenet_imagebind_embeddings.npy'\n",
    "model = load_model(model_flag, device)\n",
    "image_text_dataset = create_dataset(dataset_flag, model=model, device=device, seed=seed, embs_input=embs_input)\n",
    "\n",
    "\n",
    "x_adv_path ='outputs/imagenet/whitebox/imagebind_jpeg/x_advs_300.npy'\n",
    "y_id_path = 'outputs/imagenet/whitebox/imagebind_jpeg/y_ids.npy'\n",
    "evaluate_jpeg(model,x_adv_path, y_id_path,'vision', image_text_dataset )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_illusions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
